<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Liz Rightmire">
<meta name="dcterms.date" content="2024-05-16">
<meta name="description" content="My final project on skin cancer image classification">

<title>Liz’s CSCI 0451 Blog - Final Project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Liz’s CSCI 0451 Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Final Project</h1>
                  <div>
        <div class="description">
          My final project on skin cancer image classification
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Liz Rightmire </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 16, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="project-blog-post" class="level1">
<h1>Project Blog Post</h1>
<section id="on-skin-cancer-identification-using-cnn-and-logistic-regresison-models" class="level3">
<h3 class="anchored" data-anchor-id="on-skin-cancer-identification-using-cnn-and-logistic-regresison-models">On skin cancer identification using CNN and Logistic Regresison models</h3>
</section>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<p>Skin cancer is the most common form of cancer. According to the Victoria department of health, over 95% of skin cancers that are dected early can be successfully treated; therefore, early detection is crucial. In our analyses, we aim to identify different types of skin conditions with image classification techniques. Using convolutional neural networks (CNN) and logistic regression (LR) models, we obtain a 75% accuracy rate for classification, where melanocytic nevi (NV) and melanoma (MEL) are the most successfully classified forms of skin condition.</p>
<p>Link to GitHub Repository: <a href="https://github.com/ShadowedSpace/cancer">link</a></p>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>In 1994, Binder et al.&nbsp;trained a neural network that successfully differentiated between melanomas, the leading cause of skin cancer deaths, and melanocytic nevus, which are generally harmless skin lesions commonly known as moles or birth marks. In 2018, a challenge hosted by the International Skin Imaging Collaboration (ISIC) lay out the task of detection and classification of skin lesions and diseases. 900 parties registered to download the data for the challenge and hundreds of groups submitted novel evaluation techniques to automate the process of diagnosing skin lesions and diseases. A study conducted on the ISIC challenge revealved that the best classification models still failed to properly classify on average over 10% of dermoscopic images and had varying abilities to generalize (Coedlla et al., 2019). As it turns out, Binder et al.’s study (1994) trained their neural network on 200 images. The dataset used by the ISIC challenge is the world’s largest public repository of dermoscopic images of skin and contains 10015 dermoscopic images. First released in 2018 by Philipp Tschandl et al., the HAM10000 (Humans Against Machines) is a novel dataset with the aim of improving the process of automated skin lesion diagnosis, as most existing demoscopic image datasets are either small in size or lacking diversity.</p>
<p>According to a recent study by Tandon et al.&nbsp;(2024), the most successful deep learning model to implement for the automation of cancer diagnoses is the convolutional neural network (CNN). CNNs are a type of neural network that are particularly effective for identifying patterns in images and audio. CNNs are best used when there are large amounts of data to train the model on (MathWorks). Before the release of the HAM10000 dataset, the diversity and size of dermatoscopic images was limited, which also limited the progression of automated skin lesion detection (Tschandl et al.&nbsp;2024).</p>
<p>In our project, we apply our knowledge of Convolutional Neural Networks (CNN) and other machine learning techniques to the HAM10000 dataset with the goal of classifying types of skin lesions.</p>
</section>
<section id="values-statement" class="level3">
<h3 class="anchored" data-anchor-id="values-statement">Values Statement</h3>
<p>We created this model as a way to explore how technologies like convolutional neural networks could be used on a real-world problem. In the state it is in now, we do not recommend using our model as a true diagnosis tool. However, if we were to continue to improve this model to higher accuracy, there is a world in which community members could upload photos of skin lesions and receive a percentage risk score that their skin lesion might benefit from analysis from a professional. That being said, an image would need to be high-quality and under professional lighting to match the image style of the images in HAM10000.</p>
<p>One of the greatest challenges in the healthcare industry is access. Geographical “deserts” exist all across the country where population healthcare needs are unmet partially or totally due to lack of adequate access (Brinzac et al). TeleHealth and WebHealth solutions are aiding these communities; a skin lesion classification algorithm could be used in such services.</p>
<p>The HAM10000 dataset was created to address the issue of small size and lack of diversity in publically available skin cancer datasets. That being said, the images were collected from the Austrian and Australian population, consisting of predominantly white individuals. We have not tested how our algorithm’s results when applied to skin lesions on darker skin tones, but it would likely underperform.</p>
</section>
<section id="materials-and-methods" class="level3">
<h3 class="anchored" data-anchor-id="materials-and-methods">Materials and Methods</h3>
<section id="data" class="level5">
<h5 class="anchored" data-anchor-id="data">Data</h5>
<p>For this project, we used the publicly available HAM10000 dataset, found on the Harvard Dataverse (Tschandl, 2023). It includes 10015 dermatoscopic images of skin lesions which were collected over a period of 20 years at two different sites, one in Austria and the other in Australia (Tschandl et al.&nbsp;2018).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.jpg" class="img-fluid figure-img"></p>
<figcaption>HAM1000 Dataset</figcaption>
</figure>
</div>
<p>Along with the image data, we used the provided metadata file that contained information such as lesion_id, image_id, dx (diagnosis), dx_type (maligninat or benign), age, sex, localization, dataset.</p>
<p>The limitations of this data were immediately obvious: a vast majority of images were from the “nv” class, and a vast majority of lesions were photographed on white skin.</p>
</section>
<section id="figure-1-frequency-of-skin-lesion-categories" class="level4">
<h4 class="anchored" data-anchor-id="figure-1-frequency-of-skin-lesion-categories">Figure 1: Frequency of Skin Lesion Categories</h4>
<p><img src="images/SkinCancer_TypeFrequency.png" title="Skin Lesion Categories" class="img-fluid"></p>
<p>This figure shows the frequency of diagnoses present in the data set. Clearly, the nv dx dominates, bringing up questions about how well a model trained on this data will classify any non-nv diagnoses.</p>
<section id="approach" class="level5">
<h5 class="anchored" data-anchor-id="approach">Approach</h5>
<p>The features of our data were images, which we reduced to a size 32x32 pixels to reduce the strain on our computers. We preprocessed each image by converting it to a 32x32x3 numpy array, where the third channel represents the RGB values of the pixels.</p>
<p>Here is an example image, with the beginning of its numpy array representation: <img src="images/imagepreprocessing.png" class="img-fluid" alt="image preprocessing"></p>
<p>The goal was to predict the column of the metadata table labeled dx, the diagnosis corresponding to each lesion. We concatenated the dx column of the metadata onto the dataframe containing the image representations. We subset our data into two sets, 80% for the training set and 20% for the test set.</p>
<p>To artificially increase the size and diversity of our training dataset, we performed various transformations (such as flips, rotations, and image cropping) to the original images – a technique known as Data Augmentation. We also eliminated half of the observations from the nv class to address the unbalanced class sizes. You may notice the example below is a bit blurry, that’s because the images are now 32x32 pixel representations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/DataAugmentation.png" class="img-fluid figure-img"></p>
<figcaption>Data Augmentation</figcaption>
</figure>
</div>
<p>Then it was time to create our model! As a baseline, we utilized SkLearn’s Logistic Regression model. We ran this model on our flattened NumPy arrays, achieving a low accuracy of 58%.</p>
<p>To try to improve this result, we turned to Convolutional Neural Network models. First, we tried a minimally connected linear model. Then we added additional ReLU and Conv2D layers to see if the additional layers would improve the accuracy. We also attempted a model with added dropout layers, and a dropout probability of 25%. To prevent the model from predicting the majority class every time (nv), we implemented class weights. After training for 50 epochs, we received mediocre accuracies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/SimpleCNNs.jpg" class="img-fluid figure-img"></p>
<figcaption>Simple CNNs</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>With initial results from simple, self-defined CNNs, we were curious if we could create an even more accurate classification model using transfer learning. Transfer learning is a technique where a pre-trained model, utilizing massive computational power to train on a very large dataset, is adapted to a new but related task with a smaller dataset. The knowledge (parameters and features) learned on the previous task is fine-tuned to the new task.</p>
<p>First, we trained an ImageNet model, as designed by Stanford reserachers in 2009.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ImageNet.png" class="img-fluid figure-img"></p>
<figcaption>ImageNet Diagram</figcaption>
</figure>
</div>
<p>As expected, harnessing the power of transfer learning paid off – training for 100 epochs took about 240 minutes but produced an accuracy of about 70% on a validation set! The accuracy steadily increased over the epochs:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ImageNetTraining.jpg" class="img-fluid figure-img"></p>
<figcaption>TrainingGraph</figcaption>
</figure>
</div>
<p>Take a look at the confusion matrix:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ImageNet_Confusion.jpg" class="img-fluid figure-img"></p>
<figcaption>ConfusionMatrix</figcaption>
</figure>
</div>
<p>The model is best at predicting nv; unfortunately, this is a benign skin lesion. It is the majority class in our model, explaining why it is predicted most often. That being said, we see better accuracy with melanoma and benign keratosis than previous models.</p>
<p>With the intention of increasing classification accuracy for melanoma, the most dangerous skin cancer, we utilized mobilenet_v2, Google’s transfer learning model from 2007.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/MobileNetV2.png" class="img-fluid figure-img"></p>
<figcaption>Mobile Net V2 Architecture</figcaption>
</figure>
</div>
<p>We were able to produce the following results on validation data. When trained for 100 epochs, accuracy this time is 75%!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mobilenet_confusion.jpg" class="img-fluid figure-img"></p>
<figcaption>ConfusionMatrix</figcaption>
</figure>
</div>
<p>We see increased accuracy for melanoma, at the expense of melanocytic nevi.</p>
</section>
<section id="concluding-discussion" class="level3">
<h3 class="anchored" data-anchor-id="concluding-discussion">Concluding Discussion</h3>
<p>We were able to achieve a 75% accuracy rate, which is a huge improvement over the baseline accuracy of 58%.</p>
<p>While we most certainly did not cure cancer, we met many of our exploration and implementation goals. We gained experience with image processing, data augmentation, class weights, convolutional neural networks, and transfer learning. Our final model has some predictive power, and we were able to explore the HAM10000 dataset in depth.</p>
<p>Our results were within the range of similarly experienced projects using the HAM10000 dataset. However, it is worth noting that one group of researchers was able to achieve an accuracy of 84.3% with an EfficientNet model, which is a more advanced model than the ones we used (Tajerian et al., 2023). Another group achieved an accuracy of 96.15 using Google’s ViT patch-32 model (Himel et al., 2024). Seeing as these groups of researchers had PhDs and we have yet to obtain a Bachelor’s degree, we are content with our results.</p>
<p>If we’d had more time and resources to continue this project, we likely would have explored some of the more advanced models mentioned above, as well as attempted to replicate some of their results. We were also limited by the size of the dataset, which, is relatively small for a machine learning project but is quite large to be processed on a personal laptop. We had to reduce the sizes of the images to achieve a realistic runtime! We would have liked to further explore some of the ethical implications of our model, such as the potential for racial bias in the data and the potential for our model to be used as a diagnostic tool in underserved communities.</p>
</section>
<section id="group-contribution-statement" class="level3">
<h3 class="anchored" data-anchor-id="group-contribution-statement">Group Contribution Statement</h3>
<p>Our group was formed because the topics proposed by Liz and Zoe had similarities regarding classification in the health space. We decided to split the workload so that Liz and Zoe work primarily on the building the convolutional neural network, while Julia and Breanna worked on improving the logistic regression models. For the final blog post, Breanna wrote the abstract, introduction and group contribution statement. Julia summarized the concluding thoughts, Liz wrote the values statement and worked on the results section jointly with Zoe, and Zoe championed the data/methods section.</p>
</section>
<section id="personal-reflection" class="level3">
<h3 class="anchored" data-anchor-id="personal-reflection">Personal Reflection</h3>
<p>This project turned out to be more challenging than I’d anticipated. Image classification isn’t easy! I set out in blind hope of creating high-accuracy models like we’d found evidence of in scientific papers, but our strongest fell a little short with 75% accuracy. That being said, this is in the range of other results we found online. I do feel proud when I think about what we accomplished. Wrangling complex image data, designing and implementing multiple neural networks, and consistently improving model accuracy … that’s worth celebrating!</p>
<p>Even without the complex problem at hand, working on a team has challenges. I learned the value of getting started early, checking in with group members on their progress, and not being afraid to take a leadership role.</p>
<p>I will take the interpersonal learnings from this group project experience into my next semester and life beyond. I am much more confident now setting ambitious goals and accomplishing them. In a dream world, I hope my career will involve using Artificial Intelligence and Machine Learning to improve health outcomes. This project was a perfect opportunity to dip my toe into this area!</p>
</section>
<section id="sources" class="level3">
<h3 class="anchored" data-anchor-id="sources">Sources</h3>
<p>Binder, M., A. Steiner, M. Schwarz, S. Knollmayer, K. Wolff, and H. Pehamberger. 1994. “Application of an Artificial Neural Network in Epiluminescence Microscopy Pattern Analysis of Pigmented Skin Lesions: A Pilot Study.” British Journal of Dermatology 130 (4): 460–65. https://doi.org/10.1111/j.1365-2133.1994.tb03378.x.</p>
<p>Codella, Noel, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, et al.&nbsp;2019. “Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC).” arXiv. https://doi.org/10.48550/arXiv.1902.03368.</p>
<p>Services, Department of Health &amp; Human. n.d. “Melanoma.” Department of Health &amp; Human Services. Accessed May 13, 2024. http://www.betterhealth.vic.gov.au/health/conditionsandtreatments/melanoma.</p>
<p>Himel, Galib Muhammad Shahriar et al.&nbsp;“Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-Based Noninvasive Digital System.” International journal of biomedical imaging vol.&nbsp;2024 3022192. 3 Feb.&nbsp;2024, doi:10.1155/2024/3022192. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10858797/.</p>
<p>“Skin Cancer Early Detection.” n.d. Fred Hutch. Accessed May 13, 2024. https://www.fredhutch.org/en/patient-care/prevention/skin-cancer-early-detection.html.</p>
<p>Tajerian, Amin et al.&nbsp;“Design and validation of a new machine-learning-based diagnostic tool for the differentiation of dermatoscopic skin cancer images.” PloS one vol.&nbsp;18,4 e0284437. 14 Apr.&nbsp;2023, doi:10.1371/journal.pone.0284437. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10104315/.</p>
<p>Tandon, Ritu, Shweta Agrawal, Narendra Pal Singh Rathore, Abhinava K. Mishra, and Sanjiv Kumar Jain. 2024. “A Systematic Review on Deep Learning-Based Automated Cancer Diagnosis Models.” Journal of Cellular and Molecular Medicine 28 (6): e18144. https://doi.org/10.1111/jcmm.18144.</p>
<p>Tschandl, Philipp. 2023. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Harvard Dataverse. https://doi.org/10.7910/DVN/DBW86T.</p>
<p>Tschandl, Philipp, Cliff Rosendahl, and Harald Kittler. 2018. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Scientific Data 5 (1): 180161. https://doi.org/10.1038/sdata.2018.161.</p>
<p>“What Is a Convolutional Neural Network? | 3 Things You Need to Know.” n.d. Accessed May 13, 2024. https://www.mathworks.com/discovery/convolutional-neural-network.html.</p>
<p>Brinzac, Monica, Kuhlmann, Ellen, Dussault, Gilles. “Defining medical deserts – an international consensus-building exercise.” PubMed, National Center for Biotechnology Information. https://pubmed.ncbi.nlm.nih.gov/37421651/#:~:text=Results%3A%20The%20agreed%20definition%20highlight.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>